{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the pairs for Undercover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import random\n",
    "import faiss\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "fasttext.util.download_model('fr', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_root(lemme, cgram):\n",
    "    base = lemme.lower()\n",
    "    \n",
    "    if cgram == 'NOM':\n",
    "        return base\n",
    "    \n",
    "    if cgram == 'VER':\n",
    "        base = re.sub(r'(er|ir|re)$', '', base)\n",
    "        return base\n",
    "    \n",
    "    if cgram == 'ADJ':\n",
    "        base = re.sub(r'(Ã©|Ã©e|i|ant)$', '', base)\n",
    "        return base\n",
    "    \n",
    "    return base\n",
    "\n",
    "def are_similar(word1, word2, threshold=0.8):\n",
    "    if not word1.has_vector or not word2.has_vector:\n",
    "        return False\n",
    "    return word1.similarity(word2) > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lexique.tsv\", sep='\\t')\n",
    "\n",
    "# Keep relevant columns\n",
    "columns_to_keep = ['lemme', 'cgram', 'freqlemfilms2', 'freqlemlivres']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Merge frequencies\n",
    "df['freq'] = df['freqlemfilms2'] + df['freqlemlivres']\n",
    "df = df.drop(columns=['freqlemfilms2', 'freqlemlivres'])\n",
    "\n",
    "# Keep only one line per lemme with the biggest freq_total\n",
    "df = df.loc[df.groupby('lemme')['freq'].idxmax()]\n",
    "\n",
    "# Keep values we like\n",
    "df = df[df['freq'] >= 10]\n",
    "df = df[df['lemme'].str.len() >= 3]\n",
    "df = df[df['cgram'].isin(['NOM', 'ADJ'])]\n",
    "\n",
    "# Remove words from the same family\n",
    "df['root'] = df.apply(lambda row: family_root(row['lemme'], row['cgram']), axis=1)\n",
    "priority = {'NOM': 0, 'ADJ': 1}\n",
    "df['priority'] = df['cgram'].map(priority)\n",
    "df = df.sort_values(by=['root', 'priority', 'freq'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(subset=['root'], keep='first')\n",
    "df = df.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['lemme'].tolist()\n",
    "pairs_file = 'app/src/main/assets/pairs.txt'\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = np.array([ft.get_word_vector(w) for w in words], dtype='float32')\n",
    "\n",
    "# Normalize embeddings (important for cosine similarity with FAISS)\n",
    "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product ~ cosine similarity after normalization\n",
    "index.add(embeddings)\n",
    "\n",
    "# Search for top k neighbors for each word\n",
    "k = 300  # adjust based on how many \"similar\" words you want per word\n",
    "D, I = index.search(embeddings, k)  # D=similarities, I=indices of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered pairs\n",
    "threshold_min = 0.43  # not too unrelated\n",
    "\n",
    "with open(\"app/src/main/assets/pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, word in enumerate(words):\n",
    "        for j, sim in zip(I[i], D[i]):\n",
    "            if i >= j:  # avoid duplicate pairs and self-pair\n",
    "                continue\n",
    "            if threshold_min < sim:\n",
    "                # Only write if cgram is the same (type of word)\n",
    "                if df.loc[i, 'cgram'] == df.loc[j, 'cgram']:\n",
    "                    pair = [word.capitalize(), words[j].capitalize()]\n",
    "                    f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10  # number of random pairs to print\n",
    "\n",
    "with open(\"app/src/main/assets/pairs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pairs = [json.loads(line) for line in f]\n",
    "\n",
    "random_pairs = random.sample(pairs, min(x, len(pairs)))\n",
    "print(f\"{x} random pairs out of {len(pairs):,}:\")\n",
    "max_len = max(len(pair[0]) for pair in random_pairs) + 5\n",
    "\n",
    "for pair in random_pairs:\n",
    "    print(f\"{pair[0]:<{max_len}}{pair[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
