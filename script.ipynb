{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "tqdm.pandas()\n",
    "nlp = spacy.load(\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_root(lemme, cgram):\n",
    "    base = lemme.lower()\n",
    "    \n",
    "    if cgram == 'NOM':\n",
    "        return base  # keep noun as-is\n",
    "    \n",
    "    # Remove common verb endings\n",
    "    if cgram == 'VER':\n",
    "        # infinitive endings\n",
    "        base = re.sub(r'(er|ir|re)$', '', base)\n",
    "        return base\n",
    "    \n",
    "    # Remove participle/adjective endings\n",
    "    if cgram == 'ADJ':\n",
    "        base = re.sub(r'(é|ée|i|ant)$', '', base)\n",
    "        return base\n",
    "    \n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lexique.tsv\", sep='\\t')\n",
    "\n",
    "# Delete useless columns\n",
    "columns_to_keep = ['lemme', 'cgram', 'freqlemfilms2', 'freqlemlivres']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Replace freq columns by one wich is the addition of the two\n",
    "df['freq'] = df['freqlemfilms2'] + df['freqlemlivres']\n",
    "df = df.drop(columns=['freqlemfilms2', 'freqlemlivres'])\n",
    "\n",
    "# Keep only one line per lemme with the biggest freq_total\n",
    "df = df.loc[df.groupby('lemme')['freq'].idxmax()]\n",
    "\n",
    "# Delete all lines with low freq\n",
    "df = df[df['freq'] >= 10]\n",
    "\n",
    "# Remove small word\n",
    "df = df[df['lemme'].str.len() >= 3]\n",
    "\n",
    "# Keep only verbs, nouns and adjectives\n",
    "df = df[df['cgram'].isin(['VER', 'NOM', 'ADJ'])]\n",
    "\n",
    "# Remove words from the same family\n",
    "df['root'] = df.progress_apply(lambda row: family_root(row['lemme'], row['cgram']), axis=1)\n",
    "\n",
    "# Sort by cgram priority and freq\n",
    "priority = {'NOM': 0, 'VER': 1, 'ADJ': 2}\n",
    "df['priority'] = df['cgram'].map(priority)\n",
    "df = df.sort_values(by=['root', 'priority', 'freq'], ascending=[True, True, False])\n",
    "\n",
    "# Keep only the first occurrence of each root\n",
    "df = df.drop_duplicates(subset=['root'], keep='first')\n",
    "\n",
    "# Clean up\n",
    "df = df.drop(columns=['root', 'priority']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "def are_similar(word1, word2, threshold=0.5):\n",
    "    return word1.similarity(word2) > threshold\n",
    "\n",
    "words = df['word'].tolist()\n",
    "\n",
    "for i in range(len(words)):\n",
    "    token1 = nlp(words[i])\n",
    "    for j in range(i + 1, len(words)): \n",
    "        token2 = nlp(words[j])\n",
    "        if are_similar(token1, token2):\n",
    "            pairs.append(f\"{words[i]} - {words[j]}\")\n",
    "\n",
    "random.shuffle(pairs)\n",
    "\n",
    "with open(\"pairs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in pairs:\n",
    "        f.write(pair + \"\\n\")\n",
    "\n",
    "print(f\"Generated {len(pairs)} word pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
